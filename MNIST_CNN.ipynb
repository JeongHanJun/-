{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNt9xNSgCiyZnrR/fvbCon0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongHanJun/-/blob/master/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd5OPIqDRGV2"
      },
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "# mnist = inputdata.read_data_sets(\"./mnist/data/\", one_hot = True)\n",
        "\n",
        "# X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "# Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# # 16개의 필터를 가진 4x4 크기의 컨볼루션 계층 정의\n",
        "# W1 = tf.Variable(tf.random_normal([4, 4, 1, 16], stddev = 0.01))\n",
        "# L1 = tf.nn.conv2d(X, W1, strides = [1, 1, 1, 1], padding = 'SAME')\n",
        "# L1 = tf.nn.relu(L1)\n",
        "\n",
        "# # 2x2 pooling label\n",
        "# L1 = tf.nn.max_pool(L1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
        "\n",
        "# # 32개의 필터를 가진 4x4 크기의 컨볼루션 계층 정의\n",
        "\n",
        "# W2 = tf.Variable(tf.random_normal([4, 4, 16, 32], stddev = 0.01))\n",
        "# L2 = tf.nn.conv2d(L1, W2, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
        "# L2 = tf.nn.relu(L2)\n",
        "\n",
        "# W3 = tf.Variable(tf.random_normal([7*7*32, 256], stddev = 0.01))\n",
        "# L3 = tf.reshape(L2, [-1, 7*7*32])\n",
        "# L3 = tf.matmul(L3, W3)\n",
        "# L3 = tf.nn.relu(L3)\n",
        "# L3 = tf.nn.dropout(L3, keep_prob)\n",
        "\n",
        "# W4 = tf.Variable(tf.random_normal([256, 10], stddev = 0.01))\n",
        "# model = tf.matnul(L3, W4)\n",
        "\n",
        "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels = Y))\n",
        "# optimizer = tf.train.AdamOprimizer(0.001).minimize(cost)\n",
        "\n",
        "# init = tf.global_variables_initializer()\n",
        "# sess = tf.Session()\n",
        "# sess.run(init)\n",
        "\n",
        "# batch_size = 100\n",
        "# total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     total_cost = 0\n",
        "\n",
        "#     for i in range(total_batch):\n",
        "#         batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "#         batch_x = batch_x.reshape(-1, 28, 28, 1), cost_val = sess.run([optimizer, cost],feed_dict = {X: batch_x, Y: batch_y, keep_prob : 0.8})\n",
        "#         total_cost += cost.val\n",
        "\n",
        "#         print('반복:', '%04d' % (epoch+1), '평균비용:', '{:.4f}'.format(total_cost / total_batch))\n",
        "\n",
        "#         print(\"학습완료\")\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils\n",
        "import torch.optim as optim\n",
        " \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        " \n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow as tf\n",
        " \n",
        "class NN(nn.Module):    #NN = Neural Network = 신경망\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = tf.placeholder(tf.float32, shape = [None, 28*28])\n",
        "        self.fc2 = tf.placeholder(tf.float32, shape = [None, 10])\n",
        "        self.fc3 = tf.placeholder(tf.)\n",
        "       \n",
        " \n",
        "net= NN()\n",
        " \n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "X_input = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
        "# shape (?, 28, 28, 1)\n",
        "\n",
        "W1 = tf.get_variable('W1', shape=[5, 5, 1, 32])\n",
        "# shape (5, 5, 1, 32)\n",
        "\n",
        "L1 = tf.nn.conv2d(X_input, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "# shape (?, 28, 28, 32)\n",
        "\n",
        "L1 = tf.nn.relu(L1)\n",
        "# shape (?, 28, 28, 32)\n",
        "\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "# shape (?, 14, 14, 32)\n",
        "\n",
        "W2 = tf.get_variable('W2', shape=[5, 5, 32, 64])\n",
        "# shape (5, 5, 32, 64)\n",
        "\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "# shape (?, 14, 14, 64)\n",
        "\n",
        "L2 = tf.nn.relu(L2)\n",
        "# shape (?, 14, 14, 64)\n",
        "\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "# shape (?, 7, 7, 64)\n",
        "\n",
        "# fully-connected를 위한 matrix flatten\n",
        "L2 = tf.layers.flatten(L2) \n",
        "\n",
        "# fully-connected\n",
        "W3 = tf.get_variable('W3', shape=[7*7*64, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([1024], stddev=0.01))\n",
        "L3 = tf.matmul(L2, W3) + b3\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "W4 = tf.get_variable('W4', shape=[1024, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([10], stddev=0.01))\n",
        "logit = tf.matmul(L3, W4) + b4\n",
        "\n",
        "hypothesis = tf.nn.softmax(logit)\n",
        "\n",
        "\n",
        "learning_rate = 0.01    # 학습률 0.01로 임의 지정 \n",
        "batch_size =100         # batch 는 한번에 처리하는 사진의 수 batch_size = 100 이면 한번에 100장씩 처리한다는 뜻\n",
        " \n",
        "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum= 0.9)  # 확률적 경사 하강범 ( momentom = 양수의 실수값, 가속도)  w += lr * gradient\n",
        "criterion= nn.NLLLoss()         # NLLlose log_softMax 에 대한 결과값 (교차 엔트로피 손실 연산?)\n",
        "\n",
        "train_set = torchvision.datasets.FashionMNIST(  # data set을 받아옴\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        " \n",
        "loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size)    # 위의 data를 받아옴\n",
        " \n",
        "epochs = 10  # epoch는 전체 data set에 대해 몇번 학습(forward)을 돌릴것인가\n",
        " \n",
        "pd_results= []  # list형태로 출력\n",
        " \n",
        "for epoch in range(epochs): # 전체학습을 epochs 만큼 반복\n",
        "    batch_idx = 0           # 변수 초기화\n",
        "    tot_num = 0\n",
        "    correct_num = 0\n",
        "    for batch in loader:\n",
        "        batch_idx += 1  # 1개씩 늘리면서 진행\n",
        " \n",
        "        images = batch[0]\n",
        "        labels = batch[1]\n",
        " \n",
        "        images = images.view(-1, 28*28) # view(-1, size) 에서 왜 -1인가? -> (batch_size, 784)\n",
        "        \n",
        "        optimizer.zero_grad()   # gradient를 0 으로 해야 반복할때 오버래핑이 안생김\n",
        " \n",
        "        net_out= net(images)\n",
        " \n",
        "        loss = criterion(net_out, labels)   # 손실률\n",
        " \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "        for i in range(len(labels)):\n",
        "            tot_num += 1\n",
        "            pred = torch.max(net_out[i], 0)[1]\n",
        "            correct_num += pred.eq(labels[i]).sum()\n",
        "        \n",
        "        if batch_idx % 100 == 0:                                # data 양이 많으니 100배수 에서 출력\n",
        "            results = OrderedDict()\n",
        "            results['epoch'] = epoch\n",
        "            results['batch_idx'] = batch_idx\n",
        "            results['loss'] = loss.item()\n",
        "            results['accuracy'] = 100.*correct_num.item()/tot_num\n",
        "            pd_results.append(results)\n",
        "            df = pd.DataFrame.from_dict(pd_results, orient = 'columns')\n",
        " \n",
        "            clear_output(wait = True)\n",
        "            display(df)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}