{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow1_w, relu, sigmoid, error, b, x->y.ipynb ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZf9W9ZVSbMHdp0zbdPez7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongHanJun/-/blob/master/Tensorflow1_w%2C_relu%2C_sigmoid%2C_error%2C_b%2C_x_%3Ey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHSvqvycgUCK",
        "outputId": "7c4da706-3622-4c3f-a600-cc8570950f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x) )\n",
        "    \n",
        "rand = tf.random.uniform([1], 0, 1) # 난수 생성 uniform( [모양], 최소값, 최대값 )\n",
        "print(rand)\n",
        "\n",
        "rnor = tf.random.normal([4], 0, 1)  # 정규 분포 생성  normal( [모양], 난수 평균값, 난수 표준편차 )\n",
        "print(rnor)\n",
        "\n",
        "# 뉴런은 입력X (input) ---(가중치 w)---> 활성화함수(func) -------> 출력 Y\n",
        "# 가중치는 결과값과 기대값의 차이를 줄여주는데에 기여\n",
        "# 활성화함수 에는 sigmoid 과 relu 가 잇다.\n",
        "# sigmoid는 최종 출력값을 0과 1 사이 값\n",
        "# relu는 0이하면 0, 0 초과시 값 그대로 \n",
        "# 함수의 결과값이 소실될 수 있으므로 relu를 많이 사용\n",
        "\n",
        "x = 0\n",
        "y = 1   # 입력값 0 , 실제값 1\n",
        "w = tf.random.normal( [1], 0, 1 )\n",
        "b = tf.random.normal( [1], 0, 1 )\n",
        "                     \n",
        "\n",
        "for i in range(1000):\n",
        "    output = sigmoid(x*w + 1*b)\n",
        "    error = y - output\n",
        "    w = w + x * 0.1 * error # 0.1 = learning rate\n",
        "    b = b + 1 * 0.1 * error # bias 편향\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(i, error, output)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0.82110333], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-1.7199335   0.2693347   0.35428664  0.29932266], shape=(4,), dtype=float32)\n",
            "99 0.11129397076920877 0.8887060292307912\n",
            "199 0.05479307415568213 0.9452069258443179\n",
            "299 0.03593254347412134 0.9640674565258787\n",
            "399 0.026641010926082176 0.9733589890739178\n",
            "499 0.021136914253351624 0.9788630857466484\n",
            "599 0.0175046116167058 0.9824953883832942\n",
            "699 0.014931121787495782 0.9850688782125042\n",
            "799 0.0130137389907794 0.9869862610092206\n",
            "899 0.011530627592411968 0.988469372407588\n",
            "999 0.010349624972865845 0.9896503750271342\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}